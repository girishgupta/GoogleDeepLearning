{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 4\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb` and `3_regularization.ipynb`, we trained fully connected networks to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters.\n",
    "\n",
    "The goal of this assignment is make the neural network convolutional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "tm2CQN_Cpwj0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'C://Users//GuptaG//Documents//ML//TF//udacity.ud730.deeplearning-master//assignments//notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # only grayscale as classifcation depend on shape rather than color\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape(\n",
    "        (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "AgQDIREv02p1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Net will be CONV>>RELU>>CONV>>RELU>>FC>>RELU>>OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "IZYv70SvvOan",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "        [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "    # Model.\n",
    "    def model(data):\n",
    "        #conv2d(input_image, width, length, channel)\n",
    "        #for CNN first and last arguement is always one\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_steps = 1501\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 50 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(image_size // 4 * image_size // 4 * depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KedKkn4EutIK"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "k = 2\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "\n",
    "    # Model.\n",
    "    def model(data):\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        pool = tf.nn.max_pool(hidden, [1, k, k, 1], [1, k, k, 1], padding='SAME')\n",
    "        conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        pool = tf.nn.max_pool(hidden, [1, k, k, 1], [1, k, k, 1], padding='SAME')\n",
    "        shape = pool.get_shape().as_list()\n",
    "        reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "\n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels = tf_train_labels))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.495328\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 7.2%\n",
      "Test accuracy: 6.6%\n",
      "Minibatch loss at step 50: 2.397137\n",
      "Minibatch accuracy: 31.2%\n",
      "Validation accuracy: 23.6%\n",
      "Test accuracy: 25.5%\n",
      "Minibatch loss at step 100: 1.679007\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 41.7%\n",
      "Test accuracy: 44.4%\n",
      "Minibatch loss at step 150: 1.046049\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 60.9%\n",
      "Test accuracy: 66.7%\n",
      "Minibatch loss at step 200: 1.210949\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 67.9%\n",
      "Test accuracy: 74.7%\n",
      "Minibatch loss at step 250: 0.907836\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 74.5%\n",
      "Test accuracy: 81.2%\n",
      "Minibatch loss at step 300: 0.960928\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 73.8%\n",
      "Test accuracy: 80.6%\n",
      "Minibatch loss at step 350: 0.632563\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 76.3%\n",
      "Test accuracy: 83.5%\n",
      "Minibatch loss at step 400: 0.786362\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 75.2%\n",
      "Test accuracy: 82.4%\n",
      "Minibatch loss at step 450: 1.003517\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 77.5%\n",
      "Test accuracy: 84.6%\n",
      "Minibatch loss at step 500: 0.657380\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 76.8%\n",
      "Test accuracy: 84.0%\n",
      "Minibatch loss at step 550: 1.196180\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 77.5%\n",
      "Test accuracy: 84.8%\n",
      "Minibatch loss at step 600: 1.560590\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 77.0%\n",
      "Test accuracy: 84.4%\n",
      "Minibatch loss at step 650: 0.677349\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 76.9%\n",
      "Test accuracy: 84.0%\n",
      "Minibatch loss at step 700: 0.970912\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 79.3%\n",
      "Test accuracy: 86.2%\n",
      "Minibatch loss at step 750: 0.322793\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 79.9%\n",
      "Test accuracy: 86.9%\n",
      "Minibatch loss at step 800: 0.853880\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.7%\n",
      "Test accuracy: 86.7%\n",
      "Minibatch loss at step 850: 0.740269\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.3%\n",
      "Test accuracy: 86.3%\n",
      "Minibatch loss at step 900: 1.192880\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.9%\n",
      "Test accuracy: 87.5%\n",
      "Minibatch loss at step 950: 0.605788\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 80.5%\n",
      "Test accuracy: 87.5%\n",
      "Minibatch loss at step 1000: 0.703798\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 80.9%\n",
      "Test accuracy: 87.5%\n",
      "Minibatch loss at step 1050: 0.555336\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 80.9%\n",
      "Test accuracy: 87.9%\n",
      "Minibatch loss at step 1100: 0.677400\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.6%\n",
      "Test accuracy: 87.3%\n",
      "Minibatch loss at step 1150: 0.315873\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 81.3%\n",
      "Test accuracy: 88.1%\n",
      "Minibatch loss at step 1200: 0.560823\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.9%\n",
      "Test accuracy: 88.7%\n",
      "Minibatch loss at step 1250: 0.652276\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.1%\n",
      "Test accuracy: 88.1%\n",
      "Minibatch loss at step 1300: 0.822444\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.5%\n",
      "Test accuracy: 88.6%\n",
      "Minibatch loss at step 1350: 0.641459\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.3%\n",
      "Test accuracy: 88.4%\n",
      "Minibatch loss at step 1400: 0.154840\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 81.9%\n",
      "Test accuracy: 88.7%\n",
      "Minibatch loss at step 1450: 0.619052\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.2%\n",
      "Test accuracy: 89.1%\n",
      "Minibatch loss at step 1500: 0.448246\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.6%\n",
      "Test accuracy: 88.3%\n",
      "Minibatch loss at step 1550: 0.598949\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.5%\n",
      "Test accuracy: 88.9%\n",
      "Minibatch loss at step 1600: 0.971151\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 82.2%\n",
      "Test accuracy: 89.0%\n",
      "Minibatch loss at step 1650: 0.409153\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 81.7%\n",
      "Test accuracy: 88.9%\n",
      "Minibatch loss at step 1700: 0.338851\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.4%\n",
      "Test accuracy: 89.7%\n",
      "Minibatch loss at step 1750: 0.514131\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.0%\n",
      "Test accuracy: 88.6%\n",
      "Minibatch loss at step 1800: 0.519346\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.3%\n",
      "Test accuracy: 89.2%\n",
      "Minibatch loss at step 1850: 0.526366\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.9%\n",
      "Test accuracy: 89.6%\n",
      "Minibatch loss at step 1900: 0.241926\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.8%\n",
      "Test accuracy: 89.6%\n",
      "Minibatch loss at step 1950: 0.242171\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.8%\n",
      "Test accuracy: 89.2%\n",
      "Minibatch loss at step 2000: 0.829894\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.4%\n",
      "Test accuracy: 89.0%\n",
      "Minibatch loss at step 2050: 0.889399\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 82.5%\n",
      "Test accuracy: 89.6%\n",
      "Minibatch loss at step 2100: 0.612423\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.9%\n",
      "Test accuracy: 89.9%\n",
      "Minibatch loss at step 2150: 0.488715\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.4%\n",
      "Test accuracy: 90.1%\n",
      "Minibatch loss at step 2200: 0.584969\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.4%\n",
      "Test accuracy: 90.1%\n",
      "Minibatch loss at step 2250: 0.336564\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 83.7%\n",
      "Test accuracy: 90.4%\n",
      "Minibatch loss at step 2300: 0.525359\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.5%\n",
      "Test accuracy: 90.3%\n",
      "Minibatch loss at step 2350: 0.304057\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.6%\n",
      "Test accuracy: 90.0%\n",
      "Minibatch loss at step 2400: 0.826971\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.6%\n",
      "Test accuracy: 90.2%\n",
      "Minibatch loss at step 2450: 0.598157\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.2%\n",
      "Test accuracy: 90.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 2500\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 50 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klf21gpbAgb-"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing CNN\n",
    "Base archietechture is INPUT => CONV => RELU => POOL => CONV => RELU => POOL => FC => RELU => FC\n",
    "With dropouts, learning rate decay, max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "num_hidden2 = 64\n",
    "k = 2\n",
    "keep_prob = 0.7\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "  \n",
    "    # Variables.\n",
    "    #Conv1 Layer 28X28\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    #Conv2 Layer 12x12\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    #FC Layer one\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal([image_size//2*image_size//2*depth, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    \n",
    "    layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_hidden2], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden2]))\n",
    "    \n",
    "    layer5_weights = tf.Variable(tf.truncated_normal([num_hidden,num_labels], stddev=0.1))\n",
    "    layer5_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "    # Model.\n",
    "    def model(data):\n",
    "        #conv2d(input_image, width, length, channel)\n",
    "        #for CNN first and last arguement is always one\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        conv = tf.add(conv, layer1_biases)\n",
    "        #conv = tf.nn.dropout(conv,keep_prob=keep_prob)\n",
    "        hidden = tf.nn.relu(conv)\n",
    "        pool = tf.nn.max_pool(hidden,[1,k,k,1], [1,k,k,1],padding='SAME')\n",
    "        \n",
    "        \n",
    "        conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        conv = tf.add(conv , layer2_biases)\n",
    "        #conv = tf.nn.dropout(conv,keep_prob=keep_prob)\n",
    "        hidden = tf.nn.relu(conv)\n",
    "        pool = tf.nn.max_pool(hidden,[1,k,k,1], [1,k,k,1],padding='SAME')\n",
    "        \n",
    "        #FC1\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        #print(shape)\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        reshape = tf.matmul(reshape,layer3_weights) + layer3_biases\n",
    "        reshape = tf.nn.dropout(reshape, keep_prob=keep_prob)\n",
    "        logits = tf.nn.relu(reshape)\n",
    "        \n",
    "        #FC2\n",
    "        logits = tf.add(tf.matmul(logits, layer4_weights), layer4_biases)\n",
    "        #logits = tf.nn.dropout(logits, keep_prob=keep_prob)\n",
    "        logits = tf.nn.relu(logits)\n",
    "        \n",
    "        return tf.matmul(logits, layer5_weights) + layer5_biases\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    learning_rate = tf.train.exponential_decay(0.5,global_step,5000,0.65,staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 4.524436\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 50: 2.210018\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 29.8%\n",
      "Minibatch loss at step 100: 1.137686\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 50.1%\n",
      "Minibatch loss at step 150: 1.104823\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 59.0%\n",
      "Minibatch loss at step 200: 1.376611\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 66.5%\n",
      "Minibatch loss at step 250: 0.747553\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 72.3%\n",
      "Minibatch loss at step 300: 1.079746\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 72.3%\n",
      "Minibatch loss at step 350: 0.481948\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 73.2%\n",
      "Minibatch loss at step 400: 0.922088\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 71.3%\n",
      "Minibatch loss at step 450: 0.968533\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 500: 0.725326\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 550: 0.927995\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 600: 1.503116\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 650: 0.590189\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 700: 0.770584\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 750: 0.199990\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 800: 0.837721\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 850: 0.640928\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 900: 1.018105\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.1%\n",
      "Minibatch loss at step 950: 0.589317\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 1000: 0.645770\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 80.1%\n",
      "Minibatch loss at step 1050: 0.507541\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 1100: 0.535053\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 1150: 0.414129\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 1200: 0.394887\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 1250: 0.711446\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 1300: 0.607444\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 1350: 0.570933\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 1400: 0.164921\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 1450: 0.608673\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 1500: 0.401907\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 1550: 0.492649\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 1600: 0.783530\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 1650: 0.165448\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 1700: 0.324856\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 1750: 0.591567\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 1800: 0.514516\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 1850: 0.662787\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 1900: 0.262753\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 1950: 0.121501\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 2000: 0.697891\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 2050: 0.736212\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 2100: 0.515608\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 2150: 0.526831\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 2200: 0.385237\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 2250: 0.343563\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 2300: 0.286305\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 2350: 0.120676\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 2400: 0.693749\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 2450: 0.759771\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 2500: 0.188255\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 2550: 0.783893\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 2600: 0.409399\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 2650: 0.447700\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 2700: 0.474584\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 2750: 0.248176\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 2800: 0.260561\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 2850: 0.790344\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 2900: 0.590190\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 2950: 0.747625\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 3000: 0.471069\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 3050: 0.024126\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 3100: 0.143214\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 3150: 0.501395\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 3200: 0.110233\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 3250: 0.284808\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 3300: 0.164947\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 3350: 0.338017\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 3400: 0.292849\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 3450: 0.645850\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 3500: 0.287337\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 3550: 0.727944\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 3600: 0.336282\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 3650: 0.723653\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 3700: 0.814898\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 3750: 0.175904\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 3800: 0.949414\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 3850: 0.191371\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 3900: 0.325395\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 3950: 0.347091\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 4000: 0.632892\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 4050: 0.483597\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 4100: 0.473507\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 4150: 0.198624\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 4200: 0.219048\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 4250: 0.358134\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 4300: 0.825913\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 4350: 0.618332\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 4400: 0.489154\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 4450: 0.353205\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 4500: 0.317153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 4550: 0.235234\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 4600: 0.285661\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 4650: 0.402335\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 4700: 0.655910\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 4750: 0.133979\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 4800: 0.142668\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 4850: 1.090491\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 4900: 0.518578\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 4950: 0.549742\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.2%\n",
      "Test accuracy: 91.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5000\n",
    "loss_vec = []\n",
    "test_result = []\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        loss_vec.append(l)\n",
    "        if (step % 50 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            temp = accuracy(valid_prediction.eval(), valid_labels)\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % temp)\n",
    "            test_result.append(temp)\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIpP4fzFUv9KXNGwEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e480566470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZEEoUAXEUkQ/x9SdkQmaOidpQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e481308e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "stps = np.arange(0,5000)\n",
    "plt.plot(stps,loss_vec)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "stps = np.arange(0,5000,50)\n",
    "plt.plot(stps,test_result)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
