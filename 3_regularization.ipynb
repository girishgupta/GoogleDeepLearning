{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plotting required for tunning of beta values\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))/ predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compue the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "image_size = 28\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    #global_step = tf.Variable(0,trainable = False)\n",
    "    #stater_learning_rate = 0.1\n",
    "    #learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,100000, 0.96, staircase=True)\n",
    "    \n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size*image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32,shape=(batch_size, num_labels))\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([image_size*image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    beta_regu = tf.placeholder(tf.float32)\n",
    "    \n",
    "    logits = tf.matmul(tf_train_dataset,weights)+ biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels)) + beta_regu * tf.nn.l2_loss(weights)\n",
    "        \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.25).minimize(loss)\n",
    "    #learning_step = (tf.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step))\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights)+biases)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights)+biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 20.868767\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 9.7%\n",
      "Minibatch loss at step 500: 4.261997\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 73.9%\n",
      "Minibatch loss at step 1000: 2.843054\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 1500: 1.889867\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 2000: 1.525040\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 2500: 1.323797\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 3000: 1.163011\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 3500: 0.943403\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 4000: 0.975975\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 4500: 0.824742\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 5000: 0.545898\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 5500: 0.874880\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 6000: 0.646294\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 6500: 0.637942\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 7000: 0.935173\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 7500: 0.657194\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 8000: 0.747048\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 8500: 0.794679\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 9000: 0.736822\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 9500: 0.516093\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 10000: 0.767841\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 82.8%\n",
      "Test accuracy: 89.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (batch_size * step) % (train_labels.shape[0]-batch_size)\n",
    "        batch_data = train_dataset[offset:(offset+batch_size),:]\n",
    "        batch_labels = train_labels[offset:(offset+batch_size),:]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regu : 1e-3}\n",
    "        #feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "        _,l,predictions = session.run([optimizer,loss,train_prediction], feed_dict = feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(),test_labels)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Finished for: 0.0%\n",
      "Finished for: 0.0%\n",
      "Finished for: 0.0%\n",
      "Finished for: 0.0%\n",
      "Finished for: 0.0%\n",
      "Finished for: 0.0%\n",
      "Finished for: 0.0%\n",
      "Finished for: 0.0%\n",
      "Finished for: 0.0%\n",
      "Finished for: 0.0%\n",
      "Finished for: 0.0%\n",
      "Finished for: 0.0%\n",
      "Finished for: 0.0%\n",
      "Finished for: 0.0%\n",
      "Finished for: 0.0%\n",
      "Finished for: 0.0%\n",
      "Finished for: 0.0%\n",
      "Finished for: 0.0%\n",
      "Finished for: 0.0%\n",
      "Finished for: 0.1%\n",
      "Finished for: 0.1%\n",
      "Finished for: 0.2%\n",
      "Finished for: 0.3%\n",
      "Finished for: 0.4%\n",
      "Finished for: 0.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "beta_val = [pow(10,i) for i in np.arange(-5,0,0.2)]\n",
    "test_accuracy = []\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for beta_regu_val in beta_val:\n",
    "        for step in range(num_steps):\n",
    "            offset = (batch_size * step) % (train_labels.shape[0]-batch_size)\n",
    "            batch_data = train_dataset[offset:(offset+batch_size),:]\n",
    "            batch_labels = train_labels[offset:(offset+batch_size),:]\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regu : beta_regu_val}\n",
    "            #feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "            _,l,predictions = session.run([optimizer,loss,train_prediction], feed_dict = feed_dict)\n",
    "        test_accuracy.append(accuracy(test_prediction.eval(),test_labels))\n",
    "        print(\"Finished for: %f%\" % beta_regu_val)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEaCAYAAAD+E0veAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b2001709e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max accuracy: 89.9% at 0.001000\n"
     ]
    }
   ],
   "source": [
    "#print(test_accuracy)\n",
    "#print(beta_val)\n",
    "plt.semilogx(beta_val,test_accuracy)\n",
    "plt.grid(True)\n",
    "plt.title(\"Regularization beta vs. test accuracy\")\n",
    "plt.xlabel(\"Beta\")\n",
    "plt.ylabel(\"Test Accruracy\")\n",
    "plt.show()\n",
    "print(\"Max accuracy: %.1f%% at %f\" % (max(test_accuracy),max(beta_val*(test_accuracy == max(test_accuracy)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "image_size = 28\n",
    "hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size,image_size*image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size,num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regu = tf.placeholder(tf.float32)\n",
    "    \n",
    "    #Define Variables\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    \n",
    "    #Define weights for hidden layer\n",
    "    weights2 = tf.Variable(tf.truncated_normal([hidden_nodes,num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    #feed forward \n",
    "    logits = tf.nn.relu(tf.matmul(tf_train_dataset,weights1) + biases1)\n",
    "    logits = tf.matmul(logits,weights2)+biases2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels)) + beta_regu *(tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    test_logit = tf.nn.relu(tf.matmul(tf_test_dataset,weights1)+biases1)\n",
    "    test_logit = tf.matmul(test_logit,weights2)+biases2\n",
    "    test_prediction = tf.nn.softmax(test_logit)\n",
    "    \n",
    "    valid_logit = tf.nn.relu(tf.matmul(tf_valid_dataset,weights1)+biases1)\n",
    "    valid_logit = tf.matmul(valid_logit,weights2)+biases2\n",
    "    valid_prediction = tf.nn.softmax(valid_logit)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 565.735229\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 32.6%\n",
      "Minibatch loss at step 500: 155.245895\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 1000: 106.734650\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 1500: 76.031868\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 2000: 54.366955\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 2500: 40.541988\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 3000: 29.246292\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 3500: 21.175112\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 4000: 15.744858\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 4500: 11.650021\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 5000: 8.383613\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 5500: 6.380248\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 6000: 4.754567\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 6500: 3.482373\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 7000: 2.759692\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 7500: 2.050972\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 8000: 1.724081\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 8500: 1.426602\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 9000: 1.151484\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 9500: 0.909193\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 10000: 0.809626\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.9%\n",
      "Test accuracy: 94.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 12001\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (batch_size * step) % (train_labels.shape[0]-batch_size)\n",
    "        batch_data = train_dataset[offset:(offset+batch_size),:]\n",
    "        batch_labels = train_labels[offset:(offset+batch_size),:]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regu : 0.000631}\n",
    "        #feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : .000631}\n",
    "        _,l,predictions = session.run([optimizer,loss,train_prediction], feed_dict = feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(),test_labels)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Finished for: 0.000010\n",
      "Finished for: 0.000016\n",
      "Finished for: 0.000025\n",
      "Finished for: 0.000040\n",
      "Finished for: 0.000063\n",
      "Finished for: 0.000100\n",
      "Finished for: 0.000158\n",
      "Finished for: 0.000251\n",
      "Finished for: 0.000398\n",
      "Finished for: 0.000631\n",
      "Finished for: 0.001000\n",
      "Finished for: 0.001585\n",
      "Finished for: 0.002512\n",
      "Finished for: 0.003981\n",
      "Finished for: 0.006310\n",
      "Finished for: 0.010000\n",
      "Finished for: 0.015849\n",
      "Finished for: 0.025119\n",
      "Finished for: 0.039811\n",
      "Finished for: 0.063096\n",
      "Finished for: 0.100000\n",
      "Finished for: 0.158489\n",
      "Finished for: 0.251189\n",
      "Finished for: 0.398107\n",
      "Finished for: 0.630957\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "beta_val = [pow(10,i) for i in np.arange(-5,0,0.2)]\n",
    "test_accuracy = []\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for beta_regu_val in beta_val:\n",
    "        for step in range(num_steps):\n",
    "            offset = (batch_size * step) % (train_labels.shape[0]-batch_size)\n",
    "            batch_dat#print(test_accuracy)\n",
    "#print(beta_val)\n",
    "plt.semilogx(beta_val,test_accuracy)\n",
    "plt.grid(True)\n",
    "plt.title(\"Regularization beta vs. test accuracy\")\n",
    "plt.xlabel(\"Beta\")\n",
    "plt.ylabel(\"Test Accruracy\")\n",
    "plt.show()\n",
    "print(\"Max accuracy: %.1f%% at %f\" % (max(test_accuracy),max(beta_val*(test_accuracy == max(test_accuracy)))))a = train_dataset[offset:(offset+batch_size),:]\n",
    "            batch_labels = train_labels[offset:(offset+batch_size),:]\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regu : beta_regu_val}\n",
    "            #feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "            _,l,predictions = session.run([optimizer,loss,train_prediction], feed_dict = feed_dict)\n",
    "        test_accuracy.append(accuracy(test_prediction.eval(),test_labels))\n",
    "        print(\"Finished for: %f\" % beta_regu_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEaCAYAAAD+E0veAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\np6IgIiLt/j9MfJ7DOh7HOAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b2105974e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max accuracy: 94.6% at 0.000631\n"
     ]
    }
   ],
   "source": [
    "#print(test_accuracy)\n",
    "#print(beta_val)\n",
    "plt.semilogx(beta_val,test_accuracy)\n",
    "plt.grid(True)\n",
    "plt.title(\"Regularization beta vs. test accuracy\")\n",
    "plt.xlabel(\"Beta\")\n",
    "plt.ylabel(\"Test Accruracy\")\n",
    "plt.show()\n",
    "print(\"Max accuracy: %.1f%% at %f\" % (max(test_accuracy),max(beta_val*(test_accuracy == max(test_accuracy)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "image_size = 28\n",
    "hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size,image_size*image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size,num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regu = tf.placeholder(tf.float32)\n",
    "    \n",
    "    #Define Variables\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    \n",
    "    #Define weights for hidden layer\n",
    "    weights2 = tf.Variable(tf.truncated_normal([hidden_nodes,num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    #feed forward \n",
    "    logits = tf.nn.relu(tf.matmul(tf_train_dataset,weights1) + biases1)\n",
    "    #Dropout takes tensor and keep_prob here keep_prob will be 0.5 we can tune this parameter\n",
    "    logits = tf.nn.dropout(logits,0.5)\n",
    "    logits = tf.matmul(logits,weights2)+biases2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels)) + beta_regu *(tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    test_logit = tf.nn.relu(tf.matmul(tf_test_dataset,weights1)+biases1)\n",
    "    test_logit = tf.matmul(test_logit,weights2)+biases2\n",
    "    test_prediction = tf.nn.softmax(test_logit)\n",
    "    \n",
    "    valid_logit = tf.nn.relu(tf.matmul(tf_valid_dataset,weights1)+biases1)\n",
    "    valid_logit = tf.matmul(valid_logit,weights2)+biases2\n",
    "    valid_prediction = tf.nn.softmax(valid_logit)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 722.574707\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 30.8%\n",
      "Test accuracy: 33.2%\n",
      "Minibatch loss at step 500: 165.949402\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 78.9%\n",
      "Test accuracy: 86.9%\n",
      "Minibatch loss at step 1000: 107.553299\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 80.7%\n",
      "Test accuracy: 87.6%\n",
      "Minibatch loss at step 1500: 76.310005\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 81.1%\n",
      "Test accuracy: 88.2%\n",
      "Minibatch loss at step 2000: 54.399429\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 82.1%\n",
      "Test accuracy: 89.3%\n",
      "Minibatch loss at step 2500: 40.778923\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 82.6%\n",
      "Test accuracy: 90.2%\n",
      "Minibatch loss at step 3000: 29.322638\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.1%\n",
      "Test accuracy: 90.4%\n",
      "Minibatch loss at step 3500: 21.149702\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 84.4%\n",
      "Test accuracy: 91.5%\n",
      "Minibatch loss at step 4000: 15.847289\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 85.1%\n",
      "Test accuracy: 91.9%\n",
      "Minibatch loss at step 4500: 11.635156\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 85.2%\n",
      "Test accuracy: 92.2%\n",
      "Minibatch loss at step 5000: 8.444057\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 85.8%\n",
      "Test accuracy: 92.7%\n",
      "Minibatch loss at step 5500: 6.384719\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 85.7%\n",
      "Test accuracy: 92.6%\n",
      "Minibatch loss at step 6000: 4.812754\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.4%\n",
      "Test accuracy: 93.1%\n",
      "Minibatch loss at step 6500: 3.491092\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 86.6%\n",
      "Test accuracy: 93.2%\n",
      "Minibatch loss at step 7000: 2.913846\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.6%\n",
      "Test accuracy: 93.5%\n",
      "Minibatch loss at step 7500: 2.194495\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.0%\n",
      "Test accuracy: 93.4%\n",
      "Minibatch loss at step 8000: 1.776347\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.1%\n",
      "Test accuracy: 93.4%\n",
      "Minibatch loss at step 8500: 1.496116\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.0%\n",
      "Test accuracy: 93.5%\n",
      "Minibatch loss at step 9000: 1.291962\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.4%\n",
      "Test accuracy: 93.7%\n",
      "Minibatch loss at step 9500: 1.002351\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.3%\n",
      "Test accuracy: 93.6%\n",
      "Minibatch loss at step 10000: 0.865932\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.2%\n",
      "Test accuracy: 93.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (batch_size * step) % (train_labels.shape[0]-batch_size)\n",
    "        batch_data = train_dataset[offset:(offset+batch_size),:]\n",
    "        batch_labels = train_labels[offset:(offset+batch_size),:]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regu : .000631}\n",
    "        #feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "        _,l,predictions = session.run([optimizer,loss,train_prediction], feed_dict = feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(),test_labels)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try 3 layer with dropout and regularization to test out and learning rate decay as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "#image_size = 28\n",
    "hidden_nodes1 = 1024\n",
    "hidden_nodes2 = 512\n",
    "hidden_nodes3 = 256\n",
    "keep_prob = 0.5\n",
    "num_labels = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size,image_size*image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size,num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regu = tf.placeholder(tf.float32)\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    \n",
    "    #Define Variables\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes1],stddev=np.sqrt(2.0 / (image_size * image_size))))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_nodes1]))\n",
    "    \n",
    "    #Define weights for hidden layer1\n",
    "    weights2 = tf.Variable(tf.truncated_normal([hidden_nodes1,hidden_nodes2],stddev=np.sqrt(2.0 / (hidden_nodes1))))\n",
    "    biases2 = tf.Variable(tf.zeros([hidden_nodes2]))\n",
    "    \n",
    "    #Define weights for hidden layer2\n",
    "    weights3 = tf.Variable(tf.truncated_normal([hidden_nodes2,hidden_nodes3],stddev=np.sqrt(2.0 / (hidden_nodes2))))\n",
    "    biases3 = tf.Variable(tf.zeros([hidden_nodes3]))\n",
    "    \n",
    "    #Define weights for hidden layer3\n",
    "    weights4 = tf.Variable(tf.truncated_normal([hidden_nodes3,num_labels],stddev=np.sqrt(2.0 / (hidden_nodes2))))\n",
    "    biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    #feed forward \n",
    "    logits = tf.nn.relu(tf.matmul(tf_train_dataset,weights1) + biases1)\n",
    "    #Dropout takes tensor and keep_prob here keep_prob will be 0.5 we can tune this parameter\n",
    "    logits = tf.nn.dropout(logits,keep_prob)\n",
    "    logits = tf.nn.relu(tf.matmul(logits,weights2)+biases2)\n",
    "    #logits = tf.nn.dropout(logits,keep_prob)\n",
    "    logits = tf.nn.relu(tf.matmul(logits,weights3)+biases3)\n",
    "    #logits = tf.nn.dropout(logits,keep_prob) \n",
    "    logits = tf.matmul(logits,weights4)+biases4\n",
    "    \n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels))\n",
    "    #+ beta_regu *(tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2)+tf.nn.l2_loss(weights3)+tf.nn.l2_loss(weights4))\n",
    "    \n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase = True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    #optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    test_logits = tf.nn.relu(tf.matmul(tf_test_dataset,weights1) + biases1)\n",
    "    test_logits = tf.nn.relu(tf.matmul(test_logits,weights2)+biases2)\n",
    "    test_logits = tf.nn.relu(tf.matmul(test_logits,weights3)+biases3) \n",
    "    test_logits = tf.matmul(test_logits,weights4)+biases4\n",
    "    test_prediction = tf.nn.softmax(test_logits)\n",
    "    \n",
    "    valid_logits = tf.nn.relu(tf.matmul(tf_valid_dataset,weights1) + biases1)\n",
    "    valid_logits = tf.nn.relu(tf.matmul(valid_logits,weights2)+biases2)\n",
    "    valid_logits = tf.nn.relu(tf.matmul(valid_logits,weights3)+biases3) \n",
    "    valid_logits = tf.matmul(valid_logits,weights4)+biases4\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.340060\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 10.8%\n",
      "Test accuracy: 11.3%\n",
      "Minibatch loss at step 500: 1.397148\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 84.0%\n",
      "Test accuracy: 90.9%\n",
      "Minibatch loss at step 1000: 1.129487\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 84.7%\n",
      "Test accuracy: 91.5%\n",
      "Minibatch loss at step 1500: 0.919824\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 85.6%\n",
      "Test accuracy: 92.3%\n",
      "Minibatch loss at step 2000: 0.737285\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.9%\n",
      "Test accuracy: 92.1%\n",
      "Minibatch loss at step 2500: 0.702466\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 86.3%\n",
      "Test accuracy: 92.8%\n",
      "Minibatch loss at step 3000: 0.805223\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.5%\n",
      "Test accuracy: 93.0%\n",
      "Minibatch loss at step 3500: 0.569511\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.2%\n",
      "Test accuracy: 93.5%\n",
      "Minibatch loss at step 4000: 0.810280\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 86.8%\n",
      "Test accuracy: 93.2%\n",
      "Minibatch loss at step 4500: 0.699734\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.0%\n",
      "Test accuracy: 94.3%\n",
      "Minibatch loss at step 5000: 0.417794\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.2%\n",
      "Test accuracy: 94.2%\n",
      "Minibatch loss at step 5500: 0.652646\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.8%\n",
      "Test accuracy: 94.2%\n",
      "Minibatch loss at step 6000: 0.563591\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.0%\n",
      "Test accuracy: 94.0%\n",
      "Minibatch loss at step 6500: 0.564319\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.3%\n",
      "Test accuracy: 94.6%\n",
      "Minibatch loss at step 7000: 0.580796\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.0%\n",
      "Test accuracy: 94.2%\n",
      "Minibatch loss at step 7500: 0.468656\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.3%\n",
      "Test accuracy: 94.3%\n",
      "Minibatch loss at step 8000: 0.587119\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.3%\n",
      "Test accuracy: 94.5%\n",
      "Minibatch loss at step 8500: 0.624694\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 89.1%\n",
      "Test accuracy: 94.8%\n",
      "Minibatch loss at step 9000: 0.579043\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.9%\n",
      "Test accuracy: 94.9%\n",
      "Minibatch loss at step 9500: 0.458532\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.6%\n",
      "Test accuracy: 94.4%\n",
      "Minibatch loss at step 10000: 0.538093\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.0%\n",
      "Test accuracy: 95.1%\n",
      "Minibatch loss at step 10500: 0.566720\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.0%\n",
      "Test accuracy: 94.9%\n",
      "Minibatch loss at step 11000: 0.563781\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.2%\n",
      "Test accuracy: 95.0%\n",
      "Minibatch loss at step 11500: 0.345403\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.8%\n",
      "Test accuracy: 94.8%\n",
      "Minibatch loss at step 12000: 0.449415\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.1%\n",
      "Test accuracy: 94.8%\n",
      "Minibatch loss at step 12500: 0.624099\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 89.8%\n",
      "Test accuracy: 95.1%\n",
      "Minibatch loss at step 13000: 0.434142\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.8%\n",
      "Test accuracy: 95.3%\n",
      "Minibatch loss at step 13500: 0.411158\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.7%\n",
      "Test accuracy: 95.3%\n",
      "Minibatch loss at step 14000: 0.318451\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.6%\n",
      "Test accuracy: 95.4%\n",
      "Minibatch loss at step 14500: 0.372070\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.6%\n",
      "Test accuracy: 95.5%\n",
      "Minibatch loss at step 15000: 0.448111\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.0%\n",
      "Test accuracy: 95.5%\n",
      "Minibatch loss at step 15500: 0.424035\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.7%\n",
      "Test accuracy: 95.1%\n",
      "Minibatch loss at step 16000: 0.487895\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.7%\n",
      "Test accuracy: 95.3%\n",
      "Minibatch loss at step 16500: 0.418674\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.1%\n",
      "Test accuracy: 95.7%\n",
      "Minibatch loss at step 17000: 0.378388\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.1%\n",
      "Test accuracy: 95.7%\n",
      "Minibatch loss at step 17500: 0.376546\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.0%\n",
      "Test accuracy: 95.8%\n",
      "Minibatch loss at step 18000: 0.458030\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.2%\n",
      "Test accuracy: 95.7%\n",
      "Minibatch loss at step 18500: 0.516330\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.2%\n",
      "Test accuracy: 95.7%\n",
      "Minibatch loss at step 19000: 0.323021\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.0%\n",
      "Test accuracy: 95.6%\n",
      "Minibatch loss at step 19500: 0.254335\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.1%\n",
      "Test accuracy: 95.6%\n",
      "Minibatch loss at step 20000: 0.349796\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.3%\n",
      "Test accuracy: 95.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (batch_size * step) % (train_labels.shape[0]-batch_size)\n",
    "        batch_data = train_dataset[offset:(offset+batch_size),:]\n",
    "        batch_labels = train_labels[offset:(offset+batch_size),:]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regu : .000631}\n",
    "        #feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "        _,l,predictions = session.run([optimizer,loss,train_prediction], feed_dict = feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(),test_labels)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
